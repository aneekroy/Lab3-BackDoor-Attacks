# -*- coding: utf-8 -*-
"""ar8002-ML for CyberSec Lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_2bGVTzhiaWUBA-D_-339KM_QLtgL2S

Download the git repo : [repo](https://github.com/csaw-hackml/CSAW-HackML-2020.git
)

and the dataset in the google colab directory :  [data](https://drive.google.com/drive/folders/13o2ybRJ1BkGUvfmQEeZqDo1kskyFywab)
"""

!git clone https://github.com/csaw-hackml/CSAW-HackML-2020.git

!mkdir /content/CSAW-HackML-2020/lab3/data/bd /content/CSAW-HackML-2020/lab3/data/cl

#All Data : https://drive.google.com/drive/folders/13o2ybRJ1BkGUvfmQEeZqDo1kskyFywab

# bd_test.h5 - https://drive.google.com/file/d/1kxNACo0qFo8QdZgtGHvaA67p4h4RcNIy/
# bd_valid.h5 - https://drive.google.com/file/d/1DRKofqVdn2ioh44M45eYZHl_XAW9r3v4/
# test.h5 - https://drive.google.com/file/d/1HpahIi-RcvtaRoly_TbuoBzWUaAjVDgt/
# valid.h5 - https://drive.google.com/file/d/1nbB5tyUVClSaFvvg3hrFW4wOUj3GtNTf/


import gdown

# Download bd_test.h5
url_bd_test = 'https://drive.google.com/uc?id=1kxNACo0qFo8QdZgtGHvaA67p4h4RcNIy'
output_bd_test = '/content/CSAW-HackML-2020/lab3/data/bd/bd_test.h5'
gdown.download(url_bd_test, output_bd_test, quiet=False)

# Download bd_valid.h5
url_bd_valid = 'https://drive.google.com/uc?id=1DRKofqVdn2ioh44M45eYZHl_XAW9r3v4'
output_bd_valid = '/content/CSAW-HackML-2020/lab3/data/bd/bd_valid.h5'
gdown.download(url_bd_valid, output_bd_valid, quiet=False)

# Download test.h5
url_test = 'https://drive.google.com/uc?id=1HpahIi-RcvtaRoly_TbuoBzWUaAjVDgt'
output_test = '/content/CSAW-HackML-2020/lab3/data/cl/test.h5'
gdown.download(url_test, output_test, quiet=False)

# Download valid.h5
url_valid = 'https://drive.google.com/uc?id=1nbB5tyUVClSaFvvg3hrFW4wOUj3GtNTf'
output_valid = '/content/CSAW-HackML-2020/lab3/data/cl/valid.h5'
gdown.download(url_valid, output_valid, quiet=False)

import h5py  # Importing the h5py library to handle HDF5 files
import numpy as np  # Importing numpy for numerical operations
import matplotlib.pyplot as plt  # Importing matplotlib for data visualization

# Function to load data from the given file path
def load_dataset_from_file(file_path):
    print(file_path)
    data_file = h5py.File(file_path, 'r')  # Opening the HDF5 file
    images = np.array(data_file['data'])  # Extracting the image data
    labels = np.array(data_file['label'])  # Extracting the corresponding labels
    images = images.transpose((0,2,3,1))  # Rearranging the axis for proper image format

    return images, labels

# Base path for datasets
base_dataset_path = '/content/CSAW-HackML-2020/lab3'

# Loading clean validation and test datasets
clean_validation_images, clean_validation_labels = load_dataset_from_file(base_dataset_path + '/data/cl/valid.h5')
clean_test_images, clean_test_labels = load_dataset_from_file(base_dataset_path + '/data/cl/test.h5')

# Loading poisoned validation and test datasets
poisoned_validation_images, poisoned_validation_labels = load_dataset_from_file(base_dataset_path + '/data/bd/bd_valid.h5')
poisoned_test_images, poisoned_test_labels = load_dataset_from_file(base_dataset_path + '/data/bd/bd_test.h5')

# Visualizing the clean and poisoned data
plt.figure(figsize=(10, 4))  # Setting the figure size for visualization

# Visualizing one example of clean data
plt.subplot(1, 2, 1)  # Creating a subplot for clean data
random_index = np.random.randint(clean_validation_images.shape[0])  # Selecting a random index
selected_clean_image, selected_clean_label = clean_validation_images[random_index], clean_validation_labels[random_index]
plt.title(f'Clean Data (Label: {selected_clean_label})')  # Setting title with the label
plt.imshow(selected_clean_image / 255)  # Normalizing and displaying the image
plt.axis('off')  # Hiding the axis for a cleaner look

# Visualizing one example of poisoned data
plt.subplot(1, 2, 2)  # Creating a subplot for poisoned data
selected_poisoned_image, selected_poisoned_label = poisoned_validation_images[random_index], poisoned_validation_labels[random_index]
plt.title(f'Poisoned Data (Label: {selected_poisoned_label})')  # Setting title with the label
plt.imshow(selected_poisoned_image / 255)  # Normalizing and displaying the image
plt.axis('off')  # Hiding the axis for a cleaner look

plt.show()  # Displaying the combined plot

import keras
import numpy as np
from tqdm import tqdm

class RefinedModel(keras.Model):
    """
    A custom Keras model that outputs a repaired model "G" based on OriginalModel and PrunedModel.

    Args:
        OriginalModel (keras.Model): The original model.
        PrunedModel (keras.Model): The pruned model.

    Returns:
        keras.Model: A custom model that produces refined predictions.
    """
    def __init__(self, OriginalModel, PrunedModel):
        super(RefinedModel, self).__init__()
        self.OriginalModel = OriginalModel
        self.PrunedModel = PrunedModel

    def call(self, input_data):
        """
        Perform forward pass through the RefinedModel.

        Args:
            input_data (numpy.ndarray): Input data to the model.

        Returns:
            numpy.ndarray: Refined model predictions.
        """
        # Get predictions from the OriginalModel and PrunedModel
        original_predictions = np.argmax(self.OriginalModel(input_data), axis=1)
        pruned_predictions = np.argmax(self.PrunedModel(input_data), axis=1)

        # Combine the predictions based on certain conditions
        combined_result = np.array([original_predictions[i] if original_predictions[i] == pruned_predictions[i] else 1283 for i in range(original_predictions.shape[0])])

        # Create a result matrix with N+1 classes (1284 in this case)
        result_matrix = np.zeros((original_predictions.shape[0], 1284))
        result_matrix[np.arange(combined_result.size), combined_result] = 1

        return result_matrix

import keras

def extract_intermediate_layer_output(model, layer_index):
    """
    Create a new model that outputs the activations of the specified intermediate layer.

    Args:
        model (keras.Model): The original model.
        layer_index (int): Index of the target intermediate layer.

    Returns:
        keras.Model: A new model that outputs activations of the specified layer.
    """
    intermediate_layer = model.layers[layer_index]
    intermediate_output = intermediate_layer.output
    intermediate_model = keras.Model(inputs=model.input, outputs=intermediate_output)
    return intermediate_model

# Function to apply pruning defense mechanism

import keras
import numpy as np
from tqdm import tqdm
import keras.backend as K

def apply_pruning_defense(threshold, model_path, clean_images_valid, clean_labels_valid, poisoned_images_valid, poisoned_labels_valid, clean_images_test, clean_labels_test):
    """
    Apply pruning defense mechanism to mitigate BadNet attacks.

    Args:
        threshold (float): The threshold to stop pruning if accuracy drop exceeds.
        model_path (str): Path to the neural network model.
        clean_images_valid (numpy.ndarray): Clean validation images.
        clean_labels_valid (numpy.ndarray): Clean validation labels.
        poisoned_images_valid (numpy.ndarray): Poisoned validation images.
        poisoned_labels_valid (numpy.ndarray): Poisoned validation labels.
        clean_images_test (numpy.ndarray): Clean test images.
        clean_labels_test (numpy.ndarray): Clean test labels.

    Returns:
        keras.Model: The pruned model.
    """
    # Specify the layer index for pruning
    target_layer_index = 6
    OriginalModel = keras.models.load_model(model_path)  # Load the original model
    PrunedModel = keras.models.load_model(model_path)    # Load a copy of the original model for pruning

    # Calculate and display original accuracy on clean data
    clean_labels_predicted = np.argmax(OriginalModel.predict(clean_images_valid), axis=1)
    original_accuracy = np.mean(np.equal(clean_labels_predicted, clean_labels_valid)) * 100
    print("Original accuracy on clean data: {:.2f}%".format(original_accuracy))

    # Calculate and display Attack Success Rate (ASR) of the poisoned model
    poisoned_labels_predicted = np.argmax(OriginalModel.predict(poisoned_images_valid), axis=1)
    attack_success_rate = np.mean(np.equal(poisoned_labels_predicted, poisoned_labels_valid)) * 100
    print("Attack Success Rate (ASR) of BadNet: {:.2f}%".format(attack_success_rate))

    # Get activations from the intermediate layer
    intermediate_layer_output = extract_intermediate_layer_output(OriginalModel, target_layer_index)
    activations = intermediate_layer_output.predict(clean_images_valid)

    # Compute average activations and start pruning
    average_activations = activations.mean(axis=(0, 1, 2))
    activation_indices_sorted = np.argsort(average_activations)

    # Initialize a dictionary to store pruned models
    model_list = {2: None, 4: None, 10: None}

    # Iteratively prune channels
    for index, channel in enumerate(tqdm(activation_indices_sorted)):
        # Access and zero-out the target channel in the last layer
        target_layer = PrunedModel.layers[target_layer_index - 1]
        K.set_value(target_layer.kernel[:, :, :, channel], np.zeros_like(target_layer.kernel[:, :, :, channel]))
        K.set_value(target_layer.bias[channel], np.zeros_like(target_layer.bias[channel]))

        # Recalculate accuracy after each pruning step
        pruned_accuracy = np.mean(np.equal(np.argmax(PrunedModel.predict(clean_images_valid), axis=1), clean_labels_valid)) * 100
        print('\nValidation accuracy (after pruning channel {}): {:.2f}%'.format(channel, pruned_accuracy))

        # Check for accuracy drops of 2, 4, and 10
        if model_list[2] is None and (original_accuracy - pruned_accuracy) >= 2:
            model_temp = keras.models.clone_model(PrunedModel)
            model_temp.set_weights(PrunedModel.get_weights())
            model_list[2] = model_temp
        if model_list[4] is None and (original_accuracy - pruned_accuracy) >= 4:
            model_temp = keras.models.clone_model(PrunedModel)
            model_temp.set_weights(PrunedModel.get_weights())
            model_list[4] = model_temp
        if model_list[10] is None and (original_accuracy - pruned_accuracy) >= 10:
            model_temp = keras.models.clone_model(PrunedModel)
            model_temp.set_weights(PrunedModel.get_weights())
            model_list[10] = model_temp

        # Stop pruning if the accuracy drop exceeds the threshold
        if abs(original_accuracy - pruned_accuracy) > threshold:
            break

    # Create an instance of the RefinedModel class
    refined_model = RefinedModel(OriginalModel, PrunedModel)

    # Evaluate the refined model on clean test data
    clean_test_accuracy = np.mean(np.equal(np.argmax(refined_model(clean_images_test), axis=1), clean_labels_test)) * 100
    print("Clean test accuracy of Refined Model: {:.2f}%".format(clean_test_accuracy))

    # Evaluate the refined model on poisoned validation data
    poisoned_validation_accuracy = np.mean(np.equal(np.argmax(refined_model(poisoned_images_valid), axis=1), poisoned_labels_valid)) * 100
    print("ASR of Refined Model: {:.2f}%".format(poisoned_validation_accuracy))  # Attack success rate

    return PrunedModel

# Example usage

neural_network_model_path = "/content/CSAW-HackML-2020/lab3/models/bd_net.h5"
repaired_model_threshold_2 = apply_pruning_defense(
    2,
    model_path=neural_network_model_path,
    clean_images_valid=clean_validation_images,
    clean_labels_valid=clean_validation_labels,
    poisoned_images_valid=poisoned_validation_images,
    poisoned_labels_valid=poisoned_validation_labels,
    clean_images_test=clean_test_images,
    clean_labels_test=clean_test_labels
)
repaired_model_threshold_2.save("repaired_model_threshold_2.h5")
del repaired_model_threshold_2

neural_network_model_path = "/content/CSAW-HackML-2020/lab3/models/bd_net.h5"
repaired_model_threshold_4 = apply_pruning_defense(
    4,
    model_path=neural_network_model_path,
    clean_images_valid=clean_validation_images,
    clean_labels_valid=clean_validation_labels,
    poisoned_images_valid=poisoned_validation_images,
    poisoned_labels_valid=poisoned_validation_labels,
    clean_images_test=clean_test_images,
    clean_labels_test=clean_test_labels
)
repaired_model_threshold_4.save("repaired_model_threshold_4.h5")
del repaired_model_threshold_4

neural_network_model_path = "/content/CSAW-HackML-2020/lab3/models/bd_net.h5"
repaired_model_threshold_10 = apply_pruning_defense(
    10,
    model_path=neural_network_model_path,
    clean_images_valid=clean_validation_images,
    clean_labels_valid=clean_validation_labels,
    poisoned_images_valid=poisoned_validation_images,
    poisoned_labels_valid=poisoned_validation_labels,
    clean_images_test=clean_test_images,
    clean_labels_test=clean_test_labels
)
repaired_model_threshold_10.save("repaired_model_threshold_10.h5")
del repaired_model_threshold_10

neural_network_model_path = "/content/CSAW-HackML-2020/lab3/models/bd_net.h5"
repaired_model_threshold_20 = apply_pruning_defense(
    20,
    model_path=neural_network_model_path,
    clean_images_valid=clean_validation_images,
    clean_labels_valid=clean_validation_labels,
    poisoned_images_valid=poisoned_validation_images,
    poisoned_labels_valid=poisoned_validation_labels,
    clean_images_test=clean_test_images,
    clean_labels_test=clean_test_labels
)
repaired_model_threshold_20.save("repaired_model_threshold_20.h5")
del repaired_model_threshold_20

import matplotlib.pyplot as plt

# Function to apply pruning defense and generate statistics
def apply_pruning_and_generate_plot():
    target_layer_index = 6
    OriginalModel = keras.models.load_model(neural_network_model_path)
    PrunedModel = keras.models.load_model(neural_network_model_path)

    # Evaluate the original model on clean and poisoned data
    original_accuracy = np.mean(np.equal(np.argmax(OriginalModel.predict(clean_test_images), axis=1), clean_test_labels)) * 100
    original_asr = np.mean(np.equal(np.argmax(OriginalModel.predict(poisoned_validation_images), axis=1), poisoned_validation_labels)) * 100

    # Get intermediate layer activations
    intermediate_activations = extract_intermediate_layer_output(OriginalModel, target_layer_index).predict(clean_validation_images)
    average_activations = intermediate_activations.mean(axis=(0,1,2))
    sorted_activation_indices = np.argsort(average_activations)
    total_len = len(sorted_activation_indices)

    # Lists to store results for plotting
    clean_data_accuracies = []
    poisoned_data_asrs = []
    pruning_progresses = []

    # Apply pruning iteratively
    pruning_progress = 0
    for channel in sorted_activation_indices:
        # Zeroing out the weights of the target channel
        layer_to_prune = PrunedModel.layers[target_layer_index - 1]
        K.set_value(layer_to_prune.kernel[:, :, :, channel], np.zeros_like(layer_to_prune.kernel[:, :, :, channel]))
        K.set_value(layer_to_prune.bias[channel], np.zeros_like(layer_to_prune.bias[channel]))

        # Create a refined model instance and evaluate
        refined_model = RefinedModel(OriginalModel, PrunedModel)

        clean_data_accuracy = np.mean(np.equal(np.argmax(refined_model(clean_test_images), axis=1), clean_test_labels)) * 100
        poisoned_data_asr = np.mean(np.equal(np.argmax(refined_model(poisoned_validation_images), axis=1), poisoned_validation_labels)) * 100

        # Store results for plotting
        clean_data_accuracies.append(clean_data_accuracy)
        poisoned_data_asrs.append(poisoned_data_asr)
        pruning_progresses.append(pruning_progress / total_len)
        pruning_progress = pruning_progress + 1

    # Plot the results
    plt.figure(figsize=(10, 5))
    plt.plot(pruning_progresses, clean_data_accuracies, label='Clean Data Accuracy')
    plt.plot(pruning_progresses, poisoned_data_asrs, label='Poisoned Data ASR')
    plt.xlabel('Pruning Progress')
    plt.ylabel('Percentage')
    plt.title('Effect of Pruning on Model Accuracy and ASR')
    plt.legend()
    plt.show()

    return pruning_progresses, clean_data_accuracies, poisoned_data_asrs

pruning_progresses, clean_data_accuracies, poisoned_data_asrs = apply_pruning_and_generate_plot()

!python3 CSAW-HackML-2020/lab3/eval.py CSAW-HackML-2020/lab3/data/cl/test.h5 CSAW-HackML-2020/lab3/data/bd/bd_test.h5 /content/repaired_model_threshold_2.h5

!python3 CSAW-HackML-2020/lab3/eval.py CSAW-HackML-2020/lab3/data/cl/test.h5 CSAW-HackML-2020/lab3/data/bd/bd_test.h5 /content/repaired_model_threshold_4.h5

!python3 CSAW-HackML-2020/lab3/eval.py CSAW-HackML-2020/lab3/data/cl/test.h5 CSAW-HackML-2020/lab3/data/bd/bd_test.h5 /content/repaired_model_threshold_10.h5

!python3 CSAW-HackML-2020/lab3/eval.py CSAW-HackML-2020/lab3/data/cl/test.h5 CSAW-HackML-2020/lab3/data/bd/bd_test.h5 /content/repaired_model_threshold_20.h5

"""The ineffectiveness of pruning as a defense mechanism against certain types of attacks, especially in neural networks, can be attributed to several factors:

1. **Inherent Robustness of Neural Networks**: Neural networks, particularly deep networks, are known for their redundancy and robustness. When certain channels or neurons are pruned, the network can often compensate for the loss, maintaining its function almost as before. This resilience, while beneficial for generalization and dealing with noisy inputs, can also mean that pruning does not significantly impact the network's ability to process poisoned inputs or adversarial examples.

2. **Targeted Nature of Some Attacks**: Certain attacks are crafted with the specific architecture and weights of the target neural network in mind. If the attack is designed to exploit specific vulnerabilities or features of the network, pruning might not remove these vulnerabilities. Instead, it could require a more comprehensive restructuring or retraining of the model.

3. **Insufficient Pruning**: The extent of pruning is a critical factor. If pruning is too conservative, it might not remove enough of the network's capacity to mitigate the attack. On the other hand, aggressive pruning can degrade the performance of the network on legitimate tasks. Finding the right balance is challenging and often specific to the particular model and task.

4. **Distribution Shift in Poisoned Data**: Poisoned or adversarial data often represents a significant shift from the distribution of clean data. If pruning is guided primarily by performance on clean data, it may not address the specific ways in which poisoned data manipulates the model's responses. Therefore, the pruned model might still be vulnerable to attacks crafted to exploit these distributional differences.

5. **Lack of Specificity in Pruning**: Pruning, especially when it's done uniformly or based on generic criteria like activation values, may not target the specific neurons or channels that are most influential in the adversarial context. The neurons critical for processing poisoned inputs might remain largely unaffected, leaving the network's vulnerabilities intact.

6. **Recovery of Attack Capability**: In some cases, even after pruning, the model may still be capable of learning or recovering the attack patterns during further training or fine-tuning, especially if the underlying data used for these processes is not cleansed of poisoned examples.

7. **Complexity of Neural Networks**: Modern neural networks often have millions of parameters, and the relationship between these parameters and specific model behaviors can be highly non-linear and complex. This complexity makes it difficult to predict how changes like pruning will affect the model's behavior in all scenarios, including under attack.

In conclusion, while pruning can be an effective tool for reducing model size and computational complexity, its effectiveness as a defense mechanism against attacks on neural networks is limited and can be highly dependent on the nature of the attack and the specifics of the network architecture. More comprehensive approaches might be required for robust defense against sophisticated attacks.
"""

pruning_progresses, clean_data_accuracies, poisoned_data_asrs = apply_pruning_and_generate_plot()

import csv

# Writing to CSV
with open('pruning_data.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Pruning Progress', 'Clean Data Accuracy', 'Poisoned Data ASR'])
    for progress, accuracy, asr in zip(pruning_progresses, clean_data_accuracies, poisoned_data_asrs):
        writer.writerow([progress, accuracy, asr])

from google.colab import files
files.download('/content/repaired_model_threshold_2.h5')

from google.colab import files
files.download('/content/repaired_model_threshold_4.h5')

from google.colab import files
files.download('/content/repaired_model_threshold_20.h5')